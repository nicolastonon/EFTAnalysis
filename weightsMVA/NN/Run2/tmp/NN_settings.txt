Start of NN training :16-Nov-2020 (23:42:15)

OPTIONS
----------------- 
TTree --> result
eventWeightName --> 
parameterizedNN --> False
strategy --> CARL_singlePoint
nEpochs --> 20
splitTrainValTestData --> [0.7, 0.0, 0.3]
nHiddenLayers --> 20
nNeuronsAllHiddenLayers --> 50
activInputLayer --> prelu
activHiddenLayers --> prelu
use_normInputLayer --> True
use_batchNorm --> True
dropoutRate --> 0.0
regularizer --> ['', 0.0001]
optimizer --> Adam
learnRate --> 0.001
balancedClasses --> True
earlyStopping --> False
maxEventsPerClass --> 5000
nEventsTot_train --> -1
nEventsTot_val --> -1
nEventsTot_test --> -1
batchSizeClass --> 1000
refPoint --> rwgt_ctw_5
listOperatorsParam --> ['cpq3']
nPointsPerOperator --> 50
minWC --> -5
maxWC --> 5
nEventsPerPoint --> 1000
batchSizeEFT --> 512
score_lossWeight --> 1
regress_onLogr --> False
targetVarIdx --> []
comparVarIdx --> -1
cuts --> is_signal_SR
useHardCodedListInputFeatures --> False
useLowLevelFeatures --> False
makeValPlotsOnly --> False
testToy1D --> False
storeInTestDirectory --> True
storePerOperatorSeparately --> True
useFakeableNPL --> True
displayImages --> False
trainAtManyEFTpoints --> False
regress --> False
nofOutputNodes --> 1
maxEvents --> -1
batchSize --> 1000
samplesType --> onlySMEFT
loss --> binary_crossentropy
metrics --> binary_accuracy
NN_strategy --> MVA_EFT
----------------- 

