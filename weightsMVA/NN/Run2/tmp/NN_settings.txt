Start of NN training :24-Jan-2021 (22:39:42)

OPTIONS
----------------- 
TTree --> result
eventWeightName --> 
parameterizedNN --> False
strategy --> CARL
nEpochs --> 20
splitTrainValTestData --> [0.7, 0.0, 0.3]
nHiddenLayers --> 3
nNeuronsAllHiddenLayers --> 100
activInputLayer --> relu
activHiddenLayers --> relu
use_normInputLayer --> True
use_batchNorm --> True
dropoutRate --> 0.5
regularizer --> ['l2', 0.0001]
optimizer --> Adam
learnRate --> 0.001
balancedClasses --> True
earlyStopping --> True
maxEventsPerClass --> -1
nEventsTot_train --> -1
nEventsTot_val --> -1
nEventsTot_test --> -1
batchSizeClass --> 1000
refPoint --> SM
listOperatorsParam --> ['ctz', 'ctw', 'cpq3']
nPointsPerOperator --> 20
minWC --> -10
maxWC --> 10
listMinMaxWC --> [-5, 5, -5, 5, -15, 15, -10, 10, -15, 15]
nEventsPerPoint --> 10000
batchSizeEFT --> 1000
score_lossWeight --> 0.5
regress_onLogr --> False
targetVarIdx --> []
comparVarIdx --> -1
cuts --> is_signal_SR
useHardCodedListInputFeatures --> True
useLowLevelFeatures --> False
makeValPlotsOnly --> True
testToy1D --> False
storeInTestDirectory --> True
storePerOperatorSeparately --> True
useFakeableNPL --> True
displayImages --> False
shapPlots --> False
trainAtManyEFTpoints --> True
regress --> False
nofOutputNodes --> 1
maxEvents --> 10000
batchSize --> 1000
samplesType --> onlySMEFT
loss --> binary_crossentropy
metrics --> binary_accuracy
NN_strategy --> MVA_EFT
----------------- 

Process 0 --> PrivMC_tZq
----------------- 

