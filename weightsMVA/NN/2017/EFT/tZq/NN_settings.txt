Start of NN training :03-Sep-2020 (15:53:52)

OPTIONS
----------------- 
TTree --> result
eventWeightName --> 
strategy --> CARL_singlePoint
nEpochs --> 40
splitTrainValTestData --> [0.6, 0.2, 0.2]
nHiddenLayers --> 4
nNeuronsAllHiddenLayers --> 50
activInputLayer --> tanh
activHiddenLayers --> tanh
use_normInputLayer --> True
use_batchNorm --> True
dropoutRate --> 0.2
regularizer --> ['l2', 0.0001]
optimizer --> RMSprop
learnRate --> 0.0001
balancedClasses --> False
earlyStopping --> False
maxEventsPerClass --> -1
nEventsTot_train --> -1
nEventsTot_val --> -1
nEventsTot_test --> -1
batchSizeClass --> 256
refPoint --> rwgt_ctw_5
listOperatorsParam --> ['ctz', 'ctw']
nPointsPerOperator --> 30
minWC --> -5
maxWC --> 5
nEventsPerPoint --> 3000
batchSizeEFT --> 2000
score_lossWeight --> 1
regress_onLogr --> False
targetVarIdx --> []
comparVarIdx --> -1
cuts --> is_signal_SR
useHardCodedListInputFeatures --> True
useLowLevelFeatures --> True
makeValPlotsOnly --> False
testToy1D --> False
storeInTestDirectory --> False
parameterizedNN --> False
regress --> False
nofOutputNodes --> 1
maxEvents --> -1
batchSize --> 256
samplesType --> onlySMEFT
loss --> binary_crossentropy
metrics --> binary_accuracy
NN_strategy --> MVA_EFT
----------------- 

End of NN training and evaluation :03-sept.-2020 (15:55:43)
